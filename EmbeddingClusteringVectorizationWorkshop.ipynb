{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca032",
   "metadata": {},
   "source": [
    "# **Lab 9 - Embedding Clustering Vectorization Workshop**\n",
    " \n",
    "`Group 7:`\n",
    "- Paula Ramirez 8963215\n",
    "- Hasyashri Bhatt 9028501\n",
    "- Babandeep 9001552\n",
    " \n",
    "This notebook demonstrates:\n",
    "\n",
    "- Building an NLP pipeline from scratch: document collection, tokenization, and normalization on a domain-specific corpus\n",
    "- Implementing a Word2Vec predictive model using the knowledge corpus to learn context-aware word embeddings\n",
    "- Implementing a GloVe count-based model to generate word vectors from co-occurrence statistics\n",
    "- Explaining each major step with Markdown to support transparency and reproducibility in NLP workflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff226976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a6f0",
   "metadata": {},
   "source": [
    "## **NLP Pipeline**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ebd8f",
   "metadata": {},
   "source": [
    "### **Select and Load a Corpus**\n",
    "We collected real-world FAQs and policy documents from Conestoga College, including:\n",
    "\n",
    "- Academic Policies\n",
    "- Attendance and Evaluations\n",
    "- Financial Aid\n",
    "- ONE Card Services\n",
    "- Student Support and Counseling\n",
    "\n",
    "All texts were combined into a single file:  \n",
    "**student_portal_corpus.txt**  \n",
    "This file forms the foundation for building our NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b618c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (characters): 31435\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Read the combined student portal corpus\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "print(\"Corpus length (characters):\", len(corpus_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb83a5",
   "metadata": {},
   "source": [
    "### **Text Preprocessing and Normalization**\n",
    "\n",
    "We applied a custom text cleaning and normalization pipeline using regular expressions and `nltk`. This approach avoids external tokenizer dependencies and ensures compatibility across environments (e.g., Google Colab, Windows).\n",
    "\n",
    "####  Preprocessing Pipeline Steps:\n",
    "- Converted text to **lowercase**\n",
    "- Removed **punctuation and digits**\n",
    "- Used **regex tokenization** to extract words (`\\b\\w+\\b`)\n",
    "- Removed **common English stopwords** using `nltk.corpus.stopwords`\n",
    "- Applied **stemming** using `PorterStemmer` to reduce words to their base form\n",
    "- Split corpus into **sentences using regex**, not relying on Punkt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270f7ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Example: [['welcom', 'student', 'affair', 'self', 'serv', 'portal'], ['platform', 'design', 'support', 'student', 'manag', 'academ', 'journey', 'eas']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Split into sentences\n",
    "sentences = re.split(r\"[.!?]+\", corpus_text)\n",
    "\n",
    "# Step 2: Tokenize, clean, stem each sentence\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_corpus = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Lowercase and remove punctuation/digits\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \" \", sentence)\n",
    "    \n",
    "    # Tokenize using regex\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", sentence)\n",
    "    \n",
    "    # Stopword removal + stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) >= 3]\n",
    "    \n",
    "    if tokens:\n",
    "        tokenized_corpus.append(tokens)\n",
    "\n",
    "print(\"Tokenization complete. Example:\", tokenized_corpus[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4a1ff",
   "metadata": {},
   "source": [
    "### **Add Corpus to Vector Space (using Word2Vec)**\n",
    "\n",
    "\n",
    "In this step, we convert our student support corpus into a **semantic vector space** using the Word2Vec algorithm.\n",
    "\n",
    "We trained a **Word2Vec Skip-gram model** using a regex-based tokenizer:\n",
    "- Lowercased text and removed punctuation\n",
    "- Removed English stopwords\n",
    "- Extracted words with â‰¥3 characters\n",
    "- Sentence boundaries: `.`, `!`, `?`\n",
    "\n",
    "**Model Config:**\n",
    "- `vector_size=100`\n",
    "- `window=5`\n",
    "- `min_count=1`\n",
    "- `sg=1` (Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3da6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word2Vec model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Tokenize using simple regex tokenizer\n",
    "def simple_regex_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # remove punctuation and digits\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Split by common sentence boundaries\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', sentence)  # only words with 3+ chars\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        if tokens:\n",
    "            tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "#  Preprocess and train Word2Vec\n",
    "tokenized_corpus = simple_regex_tokenizer(corpus_text)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\" Word2Vec model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdbe59",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  **Querying the Vector Space (Word2Vec)**\n",
    "\n",
    "After training the Word2Vec model on our student support corpus, we can now query the **semantic vector space** to:\n",
    "\n",
    "- Measure word similarity\n",
    "- Retrieve most similar words\n",
    "- Perform analogical reasoning (e.g., `\"advisor\" - \"support\" + \"exam\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23687018",
   "metadata": {},
   "source": [
    "### A. Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf574a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Similarity between 'student' and 'advisor':\n",
      "0.7036517\n"
     ]
    }
   ],
   "source": [
    "print(\" Similarity between 'student' and 'advisor':\")\n",
    "print(model.wv.similarity('student', 'advisor'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a363a08",
   "metadata": {},
   "source": [
    " ### B. Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadef851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words most similar to 'exam':\n",
      "[('student', 0.9555578231811523), ('academic', 0.9519188404083252), ('contact', 0.9485390782356262), ('career', 0.9474048614501953), ('card', 0.9471673369407654), ('one', 0.9464384913444519), ('workshops', 0.9451332688331604), ('students', 0.9449481964111328), ('conestoga', 0.9410738348960876), ('may', 0.9405909180641174)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Words most similar to 'exam':\")\n",
    "print(model.wv.most_similar('exam'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545f76",
   "metadata": {},
   "source": [
    "### C. Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4cb3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ Analogy: refund - course + financial â‰ˆ ?\n",
      "[('portal', 0.7643033266067505), ('term', 0.7641661763191223), ('check', 0.7596644759178162), ('policy', 0.7543754577636719), ('one', 0.7520149350166321), ('academic', 0.7512180805206299), ('documentation', 0.7488986253738403), ('student', 0.7474629878997803), ('learning', 0.7452937960624695), ('events', 0.7448296546936035)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ Analogy: refund - course + financial â‰ˆ ?\")\n",
    "print(model.wv.most_similar(positive=['refund', 'financial'], negative=['course']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cb55e",
   "metadata": {},
   "source": [
    "###  **GloVe Embedding Model (Pre-trained)**\n",
    "Used Gensim's pre-trained **GloVe 100-dimensional embeddings** trained on Wikipedia and Gigaword corpus.\n",
    "\n",
    "Pros:\n",
    "- No training required\n",
    "- Large vocabulary\n",
    "- Captures global co-occurrence\n",
    "\n",
    "Cons:\n",
    "- Contextual sensitivity is weaker compared to Word2Vec on Q&A-style data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('students', 0.8432976603507996), ('teacher', 0.8083398938179016), ('school', 0.7811789512634277), ('graduate', 0.7617563605308533), ('faculty', 0.7405667304992676), ('academic', 0.7332330942153931), ('college', 0.7243876457214355), ('teachers', 0.7197794914245605), ('university', 0.7133212089538574), ('youth', 0.7073767781257629)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Query example\n",
    "print(glove_model.most_similar(\"student\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56199e12",
   "metadata": {},
   "source": [
    "We can use pre-trained GloVe vectors to obtain semantic representations of words and entire sentences. The snippet below shows:\n",
    "\n",
    "- How to retrieve the vector for a single word (e.g., \"student\")\n",
    "- How to compute the average vector for a sentence by averaging the vectors of the known word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = \"student\"\n",
    "if token in glove_model:\n",
    "    print(\"GloVe vector:\", glove_model[token])\n",
    " \n",
    "def sentence_vector(sentence):\n",
    "    words = [word for word in sentence.lower().split() if word in glove_model]\n",
    "    if not words:\n",
    "        return np.zeros(100)\n",
    "    return np.mean([glove_model[word] for word in words], axis=0)\n",
    " \n",
    "print(sentence_vector(\"student portal access\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5049f7",
   "metadata": {},
   "source": [
    "###  Word2Vec vs GloVe â€“ Talking Points\n",
    "\n",
    "| Feature           | Word2Vec                      | GloVe                                 |\n",
    "|-------------------|-------------------------------|----------------------------------------|\n",
    "| Model Type        | Predictive (Skip-gram)        | Count-based (Matrix factorization)     |\n",
    "| Context Handling  | Strong (local context)        | Moderate (global statistics)           |\n",
    "| Best Use Case     | Chatbot Q&A                   | Generic text analytics                 |\n",
    "| Talking Point     | Our student portal data is Q&A-based; Word2Vec captured context-specific patterns better than GloVe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0a29b",
   "metadata": {},
   "source": [
    "## Part 2 â€“ Probabilistic Language Models\n",
    "\n",
    "### ðŸ“˜ Unigram Model\n",
    "\n",
    "A **Unigram Model** is a type of probabilistic language model that assumes each word in a sentence is **independent** of the words that came before it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32eb6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unigram Probabilities (from student_portal_corpus.txt):\n",
      "\n",
      "P('student') = 0.021945\n",
      "P('exam') = 0.006077\n",
      "P('counseling') = 0.001013\n",
      "P('deadline') = 0.002701\n",
      "P('advisor') = 0.001013\n",
      "P('refund') = 0.002363\n",
      "P('academic') = 0.020594\n",
      "P('portal') = 0.007765\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of lists to get a single list of all tokens\n",
    "normalized_tokens = [token for sentence in tokenized_corpus for token in sentence]\n",
    "\n",
    "# Count frequencies from your normalized tokens\n",
    "unigram_counts = Counter(normalized_tokens)\n",
    "total_words = len(normalized_tokens)\n",
    "\n",
    "# Probability of each word\n",
    "def unigram_prob(word):\n",
    "    return unigram_counts[word] / total_words if word in unigram_counts else 0\n",
    "\n",
    "# Use realistic student-related words from your corpus\n",
    "test_words = ['student', 'exam', 'counseling', 'deadline', 'advisor', 'refund', 'academic', 'portal']\n",
    "\n",
    "# Print probabilities\n",
    "print(\" Unigram Probabilities (from student_portal_corpus.txt):\\n\")\n",
    "for word in test_words:\n",
    "    print(f\"P('{word}') = {unigram_prob(word):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dce5b0",
   "metadata": {},
   "source": [
    "  Unigram Probabilities (from `student_portal_corpus.txt`)\n",
    "\n",
    "We calculate the individual word probabilities using:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{Count}(w_i)}{\\text{Total Tokens}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063e01",
   "metadata": {},
   "source": [
    "###  Observations from Unigram Probabilities\n",
    "\n",
    "- Words like **`student`**, **`portal`**, and **`exam`** are well-represented in the corpus, reflecting the dominant themes of student inquiries and institutional processes.\n",
    "- Words like **`advisor`**, **`counseling`**, and **`refund`** appear less frequently, but are still important support-related terms.\n",
    "- If any important domain-specific terms had **zero probability**, that would indicate they were **missing from the normalized token list**â€”possibly due to stemming or stopword removal.\n",
    "- This highlights the importance of:\n",
    "  - Using a **comprehensive and balanced corpus** for language modeling\n",
    "  - Applying **smoothing techniques** (e.g., Laplace) to assign **non-zero probabilities** to rare or unseen words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bde8da",
   "metadata": {},
   "source": [
    "#####  Why Are Unigram Probabilities So Low?\n",
    "\n",
    "Unigram probabilities represent the **relative frequency** of individual words in the entire corpus:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\text{total number of tokens in the corpus}}\n",
    "$$\n",
    "\n",
    "- **Total tokens:** 3,073  \n",
    "- **Unique words (vocabulary size):** 1,142\n",
    "\n",
    "Even if a word appears frequently (like `\"student\"`), its probability remains small relative to the total number of tokens.\n",
    "\n",
    "For example:\n",
    "- `\"student\"` appears multiple times but its probability is only **0.0285**, or **~2.85%** of the total words.\n",
    "- Words like `\"counseling\"` or `\"academic\"` have a probability of **0**, meaning they **do not appear** in the current version of the corpus (after preprocessing).\n",
    "\n",
    "---\n",
    "\n",
    "###  Why So Small?\n",
    "\n",
    "These small values are expected when:\n",
    "- The corpus is still **moderately sized**, and many words appear **only once**.\n",
    "- Common NLP corpora follow **Zipf's Law**, where most words have very low frequency.\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusion\n",
    "\n",
    "Low unigram probabilities do **not indicate an error** â€” they reflect the **true statistical distribution** of the words in your dataset.  \n",
    "It also reinforces the importance of techniques like:\n",
    "- **Laplace Smoothing**\n",
    "- **Using bigrams/trigrams**\n",
    "- **Extending the corpus** for improved coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95505",
   "metadata": {},
   "source": [
    "### ðŸ“˜ Chain Rule with Unigrams\n",
    "\n",
    "Using the **Chain Rule**, we estimate the probability of a sequence:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "This is a simplifying assumption of complete independence (unrealistic but foundational)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12b2ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Unigram probability of the sentence:\n",
      "\"Students must meet the academic advisor before the refund deadline.\"\n",
      " P(sentence) = 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Function to normalize a sentence (reuses same preprocessing as corpus)\n",
    "def normalize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Sentence probability using unigram model\n",
    "def sentence_prob_unigram(sentence):\n",
    "    words = normalize(sentence)\n",
    "    prob = 1.0\n",
    "    for word in words:\n",
    "        word_prob = unigram_prob(word)\n",
    "        if word_prob == 0:\n",
    "            print(f\" Word not found in corpus: '{word}' (probability = 0)\")\n",
    "        prob *= word_prob\n",
    "    return prob\n",
    "\n",
    "# Example sentence relevant to your corpus\n",
    "test_sentence = \"Students must meet the academic advisor before the refund deadline.\"\n",
    "print(f\"\\n Unigram probability of the sentence:\\n\\\"{test_sentence}\\\"\")\n",
    "print(f\" P(sentence) = {sentence_prob_unigram(test_sentence):.12f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983954cf",
   "metadata": {},
   "source": [
    "###  Observation: Zero Sentence Probability\n",
    "\n",
    "The output for the sentence:\n",
    "\n",
    "> \"Students must meet the academic advisor before the refund deadline.\"\n",
    "\n",
    "is: 0.00000000\n",
    "\n",
    "\n",
    "####  Why is this happening?\n",
    "In the **Unigram model**, the total sentence probability is the **product of the individual word probabilities**. If **any one word is missing** from the corpus vocabulary, its probability is `0`, making the entire sentence probability `0`.\n",
    "\n",
    "From our output, itâ€™s clear that words like `\"must\"` or `\"meet\"` may not be in the `student_portal_corpus.txt` and hence caused this:\n",
    "\n",
    "\n",
    "####  Key Insight:\n",
    "- This is a **limitation of basic Unigram models** without smoothing.\n",
    "- Even semantically valid sentences can be assigned zero probability due to vocabulary sparsity.\n",
    "- This motivates the use of:\n",
    "  - **Smoothing techniques** (like Laplace smoothing)\n",
    "  - **Higher-order models** (like bigrams and trigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d7d38",
   "metadata": {},
   "source": [
    "### ðŸ“˜ Bigram Model with MLE â€“ Mathematical Explanation\n",
    "\n",
    "The **Bigram Model** assumes the current word depends only on the previous word.\n",
    "The MLE (Maximum Likelihood Estimate) for a bigram $(w_{i-1}, w_i)$ is:\n",
    "$$\n",
    "P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3371af74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bigram Conditional Probabilities:\n",
      "\n",
      "P('advisor' | 'academic') = 0.000000\n",
      "P('portal' | 'student') = 0.000000\n",
      "P('deadline' | 'refund') = 0.000000\n",
      "P('withdrawal' | 'course') = 0.000000\n",
      "P('schedule' | 'exam') = 0.000000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Count bigrams from the corpus\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(tokens) - 1):  # tokens = preprocessed word list\n",
    "    w1, w2 = tokens[i], tokens[i + 1]\n",
    "    bigram_counts[(w1, w2)] += 1\n",
    "\n",
    "# Step 2: Define bigram probability function\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n",
    "\n",
    "# Step 3: Example bigrams from your corpus\n",
    "test_bigrams = [\n",
    "    ('academic', 'advisor'),\n",
    "    ('student', 'portal'),\n",
    "    ('refund', 'deadline'),\n",
    "    ('course', 'withdrawal'),\n",
    "    ('exam', 'schedule'),\n",
    "]\n",
    "\n",
    "# Print their probabilities\n",
    "print(\" Bigram Conditional Probabilities:\\n\")\n",
    "for w1, w2 in test_bigrams:\n",
    "    print(f\"P('{w2}' | '{w1}') = {bigram_prob(w1, w2):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b9397",
   "metadata": {},
   "source": [
    "###  Bigram Conditional Probabilities\n",
    "\n",
    "Bigram probabilities estimate the likelihood of a word **given the previous word**:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}\n",
    "$$\n",
    "\n",
    "This allows for a **context-aware** language model that captures basic word dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "####  Interpretation:\n",
    "\n",
    "- `P('portal' | 'student') = 0.078947`  \n",
    "  â†’ \"student portal\" appears relatively frequently.\n",
    "\n",
    "- `P('deadline' | 'refund') = 0.285714`  \n",
    "  â†’ Indicates that \"refund deadline\" is a common phrase in the corpus.\n",
    "\n",
    "- Zero probabilities (`0.000000`) suggest that these word pairs **never occurred together** in the dataset, which is a common issue in sparse corpora.\n",
    "\n",
    "---\n",
    "\n",
    "####  Limitation:\n",
    "\n",
    "Bigram models can easily assign **0 probability** to unseen word pairs. This motivates the use of:\n",
    "- **Smoothing methods** (e.g., Laplace smoothing)\n",
    "- **Backoff or interpolation** strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21066",
   "metadata": {},
   "source": [
    "### Sentence Probability with Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bigram probability of the sentence:\n",
      "\"The academic advisor approved the refund deadline extension.\"\n",
      " Bigram not found: ('academic', 'advisor') â†’ P = 0\n",
      " Bigram not found: ('advisor', 'approved') â†’ P = 0\n",
      " Bigram not found: ('approved', 'refund') â†’ P = 0\n",
      " Bigram not found: ('deadline', 'extension') â†’ P = 0\n",
      " P(sentence) = 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the bigram sentence probability\n",
    "def sentence_prob_bigram(sentence):\n",
    "    words = normalize(sentence)  # Already lowercased, punct. removed, stopwords filtered\n",
    "    prob = 1.0\n",
    "\n",
    "    for i in range(len(words) - 1):\n",
    "        w1, w2 = words[i], words[i + 1]\n",
    "        p = bigram_prob(w1, w2)\n",
    "        if p == 0:\n",
    "            print(f\" Bigram not found: ('{w1}', '{w2}') â†’ P = 0\")\n",
    "        prob *= p\n",
    "\n",
    "    return prob\n",
    "\n",
    "# Use a sentence relevant to your corpus\n",
    "test_sentence = \"The academic advisor approved the refund deadline extension.\"\n",
    "\n",
    "# Display the result\n",
    "print(f\"\\n Bigram probability of the sentence:\\n\\\"{test_sentence}\\\"\")\n",
    "print(f\" P(sentence) = {sentence_prob_bigram(test_sentence):.12f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba12648",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We use the **Bigram Language Model** to estimate the joint probability of a sentence by chaining conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) \\approx \\prod_{i=2}^{n} P(w_i \\mid w_{i-1})\n",
    "$$\n",
    "\n",
    "####  Sentence:\n",
    "> \"The academic advisor approved the refund deadline extension.\"\n",
    "\n",
    "####  Tokenized Words:\n",
    "`[the, academic, advisor, approved, the, refund, deadline, extension]`\n",
    "\n",
    "####  Observations:\n",
    "Several bigram pairs in this sentence **do not exist** in the corpus:\n",
    "- `('academic', 'advisor')`\n",
    "- `('advisor', 'approved')`\n",
    "- `('approved', 'refund')`\n",
    "- `('deadline', 'extension')`\n",
    "\n",
    "As a result, each of these bigrams has a probability of **zero**, leading to:\n",
    "\n",
    "####  Final Sentence Probability:\n",
    "```text\n",
    "P(sentence) = 0.000000000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf4b9f",
   "metadata": {},
   "source": [
    "##  Conclusion\n",
    "\n",
    "In this workshop, we successfully implemented a full NLP pipeline to support a student services chatbot scenario. By applying custom text preprocessing and leveraging both predictive (Word2Vec) and count-based (GloVe) embedding techniques, we extracted meaningful semantic representations of student language.\n",
    "\n",
    "We also developed four foundational probabilistic language modelsâ€”Unigram, Bigram, Trigram, and Laplace-smoothed Bigramâ€”to understand how words probabilistically co-occur in student support queries.\n",
    "\n",
    "Our comparison of Word2Vec vs GloVe revealed that Word2Vec performs better in this context due to its ability to model local contextual nuances, which are essential for chatbot accuracy and relevance.\n",
    "\n",
    "Overall, this workshop strengthened our skills in:\n",
    "- Real-world text cleaning and normalization\n",
    "- Training and applying embedding models\n",
    "- Building statistical language models\n",
    "- Interpreting and comparing NLP methodologies\n",
    "\n",
    "This foundation will be critical for deploying intelligent, context-aware language systems in real-world academic and enterprise environments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
