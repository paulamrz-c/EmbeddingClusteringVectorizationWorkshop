{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca032",
   "metadata": {},
   "source": [
    "# **Lab 9 - Embedding Clustering Vectorization Workshop**\n",
    " \n",
    "`Group 7:`\n",
    "- Paula Ramirez 8963215\n",
    "- Hasyashri Bhatt 9028501\n",
    "- Babandeep 9001552\n",
    " \n",
    "This notebook demonstrates:\n",
    "\n",
    "- Building an NLP pipeline from scratch: document collection, tokenization, and normalization on a domain-specific corpus\n",
    "- Implementing a Word2Vec predictive model using the knowledge corpus to learn context-aware word embeddings\n",
    "- Implementing a GloVe count-based model to generate word vectors from co-occurrence statistics\n",
    "- Explaining each major step with Markdown to support transparency and reproducibility in NLP workflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff226976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a6f0",
   "metadata": {},
   "source": [
    "## **NLP Pipeline**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ebd8f",
   "metadata": {},
   "source": [
    "### **Select and Load a Corpus**\n",
    "We collected real-world FAQs and policy documents from Conestoga College, including:\n",
    "\n",
    "- Academic Policies\n",
    "- Attendance and Evaluations\n",
    "- Financial Aid\n",
    "- ONE Card Services\n",
    "- Student Support and Counseling\n",
    "\n",
    "All texts were combined into a single file:  \n",
    "**student_portal_corpus.txt**  \n",
    "This file forms the foundation for building our NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b618c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (characters): 31435\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Read the combined student portal corpus\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "print(\"Corpus length (characters):\", len(corpus_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb83a5",
   "metadata": {},
   "source": [
    "### **Text Preprocessing and Normalization**\n",
    "\n",
    "We applied a custom text cleaning and normalization pipeline using regular expressions and `nltk`. This approach avoids external tokenizer dependencies and ensures compatibility across environments (e.g., Google Colab, Windows).\n",
    "\n",
    "####  Preprocessing Pipeline Steps:\n",
    "- Converted text to **lowercase**\n",
    "- Removed **punctuation and digits**\n",
    "- Used **regex tokenization** to extract words (`\\b\\w+\\b`)\n",
    "- Removed **common English stopwords** using `nltk.corpus.stopwords`\n",
    "- Applied **stemming** using `PorterStemmer` to reduce words to their base form\n",
    "- Split corpus into **sentences using regex**, not relying on Punkt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270f7ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Example: [['welcom', 'student', 'affair', 'self', 'serv', 'portal'], ['platform', 'design', 'support', 'student', 'manag', 'academ', 'journey', 'eas']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Split into sentences\n",
    "sentences = re.split(r\"[.!?]+\", corpus_text)\n",
    "\n",
    "# Step 2: Tokenize, clean, stem each sentence\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_corpus = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Lowercase and remove punctuation/digits\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \" \", sentence)\n",
    "    \n",
    "    # Tokenize using regex\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", sentence)\n",
    "    \n",
    "    # Stopword removal + stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) >= 3]\n",
    "    \n",
    "    if tokens:\n",
    "        tokenized_corpus.append(tokens)\n",
    "\n",
    "print(\"Tokenization complete. Example:\", tokenized_corpus[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4a1ff",
   "metadata": {},
   "source": [
    "### **Add Corpus to Vector Space (using Word2Vec)**\n",
    "\n",
    "\n",
    "In this step, we convert our student support corpus into a **semantic vector space** using the Word2Vec algorithm.\n",
    "\n",
    "We trained a **Word2Vec Skip-gram model** using a regex-based tokenizer:\n",
    "- Lowercased text and removed punctuation\n",
    "- Removed English stopwords\n",
    "- Extracted words with ≥3 characters\n",
    "- Sentence boundaries: `.`, `!`, `?`\n",
    "\n",
    "**Model Config:**\n",
    "- `vector_size=100`\n",
    "- `window=5`\n",
    "- `min_count=1`\n",
    "- `sg=1` (Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3da6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word2Vec model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Tokenize using simple regex tokenizer\n",
    "def simple_regex_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # remove punctuation and digits\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Split by common sentence boundaries\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', sentence)  # only words with 3+ chars\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        if tokens:\n",
    "            tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "#  Preprocess and train Word2Vec\n",
    "tokenized_corpus = simple_regex_tokenizer(corpus_text)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\" Word2Vec model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdbe59",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  **Querying the Vector Space (Word2Vec)**\n",
    "\n",
    "After training the Word2Vec model on our student support corpus, we can now query the **semantic vector space** to:\n",
    "\n",
    "- Measure word similarity\n",
    "- Retrieve most similar words\n",
    "- Perform analogical reasoning (e.g., `\"advisor\" - \"support\" + \"exam\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23687018",
   "metadata": {},
   "source": [
    "### A. Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf574a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Similarity between 'student' and 'advisor':\n",
      "0.7036517\n"
     ]
    }
   ],
   "source": [
    "print(\" Similarity between 'student' and 'advisor':\")\n",
    "print(model.wv.similarity('student', 'advisor'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a363a08",
   "metadata": {},
   "source": [
    " ### B. Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadef851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words most similar to 'exam':\n",
      "[('student', 0.9555578231811523), ('academic', 0.9519188404083252), ('contact', 0.9485390782356262), ('career', 0.9474048614501953), ('card', 0.9471673369407654), ('one', 0.9464384913444519), ('workshops', 0.9451332688331604), ('students', 0.9449481964111328), ('conestoga', 0.9410738348960876), ('may', 0.9405909180641174)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Words most similar to 'exam':\")\n",
    "print(model.wv.most_similar('exam'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545f76",
   "metadata": {},
   "source": [
    "### C. Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4cb3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ Analogy: refund - course + financial ≈ ?\n",
      "[('portal', 0.7643033266067505), ('term', 0.7641661763191223), ('check', 0.7596644759178162), ('policy', 0.7543754577636719), ('one', 0.7520149350166321), ('academic', 0.7512180805206299), ('documentation', 0.7488986253738403), ('student', 0.7474629878997803), ('learning', 0.7452937960624695), ('events', 0.7448296546936035)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ Analogy: refund - course + financial ≈ ?\")\n",
    "print(model.wv.most_similar(positive=['refund', 'financial'], negative=['course']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cb55e",
   "metadata": {},
   "source": [
    "###  **GloVe Embedding Model (Pre-trained)**\n",
    "Used Gensim's pre-trained **GloVe 100-dimensional embeddings** trained on Wikipedia and Gigaword corpus.\n",
    "\n",
    "Pros:\n",
    "- No training required\n",
    "- Large vocabulary\n",
    "- Captures global co-occurrence\n",
    "\n",
    "Cons:\n",
    "- Contextual sensitivity is weaker compared to Word2Vec on Q&A-style data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('students', 0.8432976603507996), ('teacher', 0.8083398938179016), ('school', 0.7811789512634277), ('graduate', 0.7617563605308533), ('faculty', 0.7405667304992676), ('academic', 0.7332330942153931), ('college', 0.7243876457214355), ('teachers', 0.7197794914245605), ('university', 0.7133212089538574), ('youth', 0.7073767781257629)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Query example\n",
    "print(glove_model.most_similar(\"student\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56199e12",
   "metadata": {},
   "source": [
    "We can use pre-trained GloVe vectors to obtain semantic representations of words and entire sentences. The snippet below shows:\n",
    "\n",
    "- How to retrieve the vector for a single word (e.g., \"student\")\n",
    "- How to compute the average vector for a sentence by averaging the vectors of the known word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = \"student\"\n",
    "if token in glove_model:\n",
    "    print(\"GloVe vector:\", glove_model[token])\n",
    " \n",
    "def sentence_vector(sentence):\n",
    "    words = [word for word in sentence.lower().split() if word in glove_model]\n",
    "    if not words:\n",
    "        return np.zeros(100)\n",
    "    return np.mean([glove_model[word] for word in words], axis=0)\n",
    " \n",
    "print(sentence_vector(\"student portal access\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5049f7",
   "metadata": {},
   "source": [
    "###  Word2Vec vs GloVe – Talking Points\n",
    "\n",
    "| Feature           | Word2Vec                      | GloVe                                 |\n",
    "|-------------------|-------------------------------|----------------------------------------|\n",
    "| Model Type        | Predictive (Skip-gram)        | Count-based (Matrix factorization)     |\n",
    "| Context Handling  | Strong (local context)        | Moderate (global statistics)           |\n",
    "| Best Use Case     | Chatbot Q&A                   | Generic text analytics                 |\n",
    "| Talking Point     | Our student portal data is Q&A-based; Word2Vec captured context-specific patterns better than GloVe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf4b9f",
   "metadata": {},
   "source": [
    "##  Conclusion\n",
    "\n",
    "In this workshop, we successfully implemented a full NLP pipeline to support a student services chatbot scenario. By applying custom text preprocessing and leveraging both predictive (Word2Vec) and count-based (GloVe) embedding techniques, we extracted meaningful semantic representations of student language.\n",
    "\n",
    "Our comparison of Word2Vec vs GloVe revealed that Word2Vec performs better in this context due to its ability to model local contextual nuances, which are essential for chatbot accuracy and relevance.\n",
    "\n",
    "Overall, this workshop strengthened our skills in:\n",
    "- Real-world text cleaning and normalization\n",
    "- Training and applying embedding models\n",
    "- Interpreting and comparing NLP methodologies\n",
    "\n",
    "This foundation will be critical for deploying intelligent, context-aware language systems in real-world academic and enterprise environments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
